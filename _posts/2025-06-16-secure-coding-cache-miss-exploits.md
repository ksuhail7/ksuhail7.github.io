# Secure Coding Practice: Mitigating Security Risks from Cache Miss Exploits

In modern applications, caching is a standard approach to reduce database load and improve response times. However, poorly designed caching strategies can inadvertently open up security risks. 
One such threat vector arises when malicious actors deliberately cause **cache misses**, forcing the application to repeatedly hit the backend database. 
This can lead to **denial of service (DoS)-like attack**, degrading application performance or even taking it offline.

In this post, we'll explore how such attacks work and explore practical strategies to mitigate the risk.

## The Problem:

Consider a typical flow in an application:

1. The application receives a request for a user.

2. It checks a cache (e.g., Redis or Memcached) for the data needed to serve the request.

3. If the required data is present (cache hit), it's used to serve the request.

4. If the data is missing (cache miss), it queries the database, processes the result, and updates the cache.

This works well under normal circumstances. What if an attacker identifies a pattern in cache keys, crafts and sends a flood of requests designed to cause cache misses?

The consequences include increased database load, denial-of-service for legitimate users.

## Mitigation Strategies

### 1. Implement Negative Caching/check for key existence

Cache the result of "not found" responses (e.g., 404 or empty database results) to prevent repeated database hits for the same invalid keys. 

**Pro Tip**: Use a **Bloom filter** as a fast and memory-efficient way to check if a key is likely to exist before hitting the cache or database. If the filter indicates the key doesn't exist, skip the lookup entirely. This can drastically reduce unnecessary database hits for bogus keys.

### 2. Reduce request locking & batching

When multiple requests try to populate the same cache key simultaneously (due to a miss), ensure only one request hits the database and the others wait. 

Or, batch multiple requests for the same key into to a single database request.

### 3. Validate cache key input

Use strict validation for cache key inputs and sanitize them. Avoid allowing user input to directly influence cache keys without checks.

### 4. Introduce Request Throttling and Rate Limiting
Limit the rate at which clients (IP, token, or session) can make requests. Enforce fair use by using NGINX rate limiting, API gateways.

### 5. Monitor Cache and DB Hit Ratios

Last, but not the least, setup observability for cache hit/miss rates and database query volume. Sudden changes in these metrics often signal ongoing abuse or inefficiency.

## Conclusion

Caching is a powerful performance optimization strategy. Without thoughtful safeguards, it can become a liability. 
The threat can be addressed with a combination of better caching practices, rate limiting, and careful validation.
